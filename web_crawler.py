"""
ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦ã‚µã‚¤ãƒˆæ§‹é€ ã‚’è§£æã™ã‚‹
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import concurrent.futures
from typing import Set, List, Dict, Optional
from collections import deque
import threading

from sitemap_data import SitemapData

class WebCrawler:
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, start_url: str, max_pages: int = 1000, max_workers: int = 10, user_agent: str = 'WebCrawler/1.0'):
        self.start_url = start_url
        self.domain = urlparse(start_url).netloc
        self.max_pages = max_pages
        self.max_workers = max_workers
        self.user_agent = user_agent
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': user_agent})
        
        self.visited: Set[str] = set()
        self.urls_to_visit: deque = deque()
        self.data = SitemapData()
        self.data.source_url = start_url
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒ­ãƒƒã‚¯
        self.lock = threading.Lock()
        
    def crawl(self) -> SitemapData:
        """ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ•·ï¸ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹: {self.start_url}")
        print(f"   ä¸Šé™ãƒšãƒ¼ã‚¸æ•°: {self.max_pages}")
        
        # åˆæœŸURLã‚’è¿½åŠ 
        self.urls_to_visit.append((self.start_url, None)) # (url, parent)
        self.visited.add(self.start_url)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            while len(self.data.urls) < self.max_pages and (self.urls_to_visit or futures):
                # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
                while self.urls_to_visit and len(futures) < self.max_workers * 2:
                    url, parent = self.urls_to_visit.popleft()
                    future = executor.submit(self._process_url, url, parent)
                    futures.append(future)
                
                if not futures:
                    break
                
                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
                done, not_done = concurrent.futures.wait(futures, timeout=0.1, return_when=concurrent.futures.FIRST_COMPLETED)
                futures = list(not_done)
                
                for future in done:
                    try:
                        new_links = future.result()
                        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                        for link in new_links:
                            with self.lock:
                                if link not in self.visited and len(self.visited) < self.max_pages:
                                    self.visited.add(link)
                                    # è¦ªURLã¯ç¾åœ¨å‡¦ç†ä¸­ã®URL
                                    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã—ãŸURLã‚’è¦ªã¨ã™ã‚‹
                                    # _process_urlå†…ã§å‡¦ç†ã—ãŸURLãŒè¦ªã«ãªã‚‹ãŒã€
                                    # ä¸¦åˆ—å‡¦ç†ã®çµæœã¨ã—ã¦è¿”ã£ã¦ãã‚‹ã®ã¯å­ãƒªãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆãªã®ã§
                                    # ã“ã“ã§è¦ªã¨ã®ç´ä»˜ã‘ãŒå°‘ã—é›£ã—ã„ã€‚
                                    # è¨­è¨ˆã‚’å¤‰æ›´ã—ã€_process_urlå†…ã§data.add_urlã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
                                    pass
                    except Exception as e:
                        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                
                print(f"\r[{len(self.data.urls)}/{self.max_pages}] Crawling...", end="", flush=True)
        
        print(f"\nâœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {len(self.data.urls)} ãƒšãƒ¼ã‚¸ç™ºè¦‹")
        return self.data

    def _process_url(self, url: str, parent: Optional[str]) -> List[str]:
        """URLã‚’å‡¦ç†ã—ã¦ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        found_links = []
        
        try:
            time.sleep(0.1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
            # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§Content-Typeç¢ºèªï¼ˆHTMLä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            try:
                head_resp = self.session.head(url, timeout=5, allow_redirects=True)
                content_type = head_resp.headers.get('Content-Type', '')
                if 'text/html' not in content_type:
                    # HTMLã§ãªã„å ´åˆã‚‚ãƒ‡ãƒ¼ã‚¿ã«ã¯è¿½åŠ ã™ã‚‹ãŒã€ãƒªãƒ³ã‚¯è§£æã¯ã—ãªã„
                    with self.lock:
                        self.data.add_url(url, status_code=head_resp.status_code, discovery_parent=parent)
                    return []
            except Exception:
                pass

            # GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.session.get(url, timeout=10)
            status_code = response.status_code
            
            with self.lock:
                self.data.add_url(url, status_code=status_code, discovery_parent=parent)
            
            if status_code != 200:
                return []
            
            # HTMLè§£æ
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ãƒªãƒ³ã‚¯æŠ½å‡º
            unique_links = set()
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                absolute_url = urljoin(url, href)
                
                # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
                absolute_url = absolute_url.split('#')[0]
                
                # ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ã¿
                if urlparse(absolute_url).netloc == self.domain:
                    # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯ï¼ˆç”»åƒãªã©ã‚’é™¤å¤–ï¼‰
                    if not any(absolute_url.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.ico', '.pdf']):
                        unique_links.add(absolute_url)
            
            # ãƒªãƒ³ã‚¯ã®ç™»éŒ²ã¨å‚ç…§å…ƒã®æ›´æ–°
            with self.lock:
                for link in unique_links:
                    # ã¾ã ãƒ‡ãƒ¼ã‚¿ã«ãªã„å ´åˆã¯è¿½åŠ 
                    if link not in self.data.url_map:
                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å¾Œã§ãƒã‚§ãƒƒã‚¯ã•ã‚Œã‚‹ãŒã€ä¸€æ—¦ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦è¿½åŠ 
                        self.data.add_url(link, discovery_parent=url)
                    
                    # å‚ç…§å…ƒã‚’è¿½åŠ 
                    self.data.add_referrer(link, url)
                    
                    # æœªè¨ªå•ãªã‚‰ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                    if link not in self.visited and len(self.visited) < self.max_pages:
                        self.visited.add(link)
                        self.urls_to_visit.append((link, url))
                        found_links.append(link)
            
            # å‚ç…§å…ƒæƒ…å ±ã®æ›´æ–°ï¼ˆæ—¢å­˜ã®URLã«å¯¾ã—ã¦ã‚‚è¡Œã†ï¼‰
            # ã“ã‚Œã¯ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šããªã‚‹ã®ã§ã€ä»Šå›ã¯ã€Œç™ºè¦‹æ™‚ã®è¦ªã€ã‚’é‡è¦–ã—ã€
            # å…¨å‚ç…§å…ƒã®è¿½è·¡ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã¾ãŸã¯åˆ¥ã®ãƒ•ã‚§ãƒ¼ã‚ºï¼‰ã¨ã™ã‚‹ã®ãŒè‰¯ã„ãŒã€
            # è¦ä»¶ã«ã‚ã‚‹ã®ã§ç°¡æ˜“çš„ã«è¿½åŠ ã—ã¦ãŠã
            # ãŸã ã—ã€WebCrawlerã‚¯ãƒ©ã‚¹å†…ã§ã¯SitemapURLã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå°‘ã—é¢å€’
            # ã“ã“ã§ã¯discovery_parentã‚’å„ªå…ˆã™ã‚‹
            
        except Exception as e:
            print(f"âŒ {url} - {e}")
            with self.lock:
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç™»éŒ²ã¯ã—ã¦ãŠã
                self.data.add_url(url, status_code=0, discovery_parent=parent)
        
        return found_links
