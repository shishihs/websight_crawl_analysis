#!/usr/bin/env python3
"""
WebSight ç·åˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
sitemap.xmlè§£æã¨ã‚¦ã‚§ãƒ–ã‚¯ãƒ­ãƒ¼ãƒ«ã®ä¸¡æ–¹ã‚’å®Ÿè¡Œ
"""

import argparse
from sitemap_analyzer import SitemapAnalyzer
from sitemap_visualizer import SitemapVisualizer
from web_crawler import WebCrawler
from sitemap_data import SitemapData

def main():
    parser = argparse.ArgumentParser(description='WebSight Total Analysis (Sitemap + Crawl)')
    parser.add_argument('--sitemap-url', type=str, required=True, help='sitemap.xmlã®URL')
    parser.add_argument('--crawl-url', type=str, help='ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹URL (æŒ‡å®šã—ãªã„å ´åˆã¯sitemapã®ã¿)')
    parser.add_argument('--max-pages', type=int, default=500, help='æœ€å¤§ã‚¯ãƒ­ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸æ•°')
    parser.add_argument('--workers', type=int, default=10, help='ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°')
    parser.add_argument('--skip-sitemap', action='store_true', help='sitemapè§£æã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--skip-crawl', action='store_true', help='ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    args = parser.parse_args()
    
    # === Sitemap.xml è§£æ ===
    if not args.skip_sitemap:
        print("=" * 60)
        print("ğŸ“„ Sitemap.xml è§£æ")
        print("=" * 60)
        print(f"ğŸ” è§£æé–‹å§‹: {args.sitemap_url}\n")
        
        analyzer = SitemapAnalyzer(check_links=True)
        sitemap_data = analyzer.analyze(args.sitemap_url)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
        sitemap_data.save_json("websight_sitemap_data.json")
        sitemap_data.save_csv("websight_sitemap_data.csv")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("\nğŸ¨ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        visualizer = SitemapVisualizer(sitemap_data)
        visualizer.generate_html_report("websight_sitemap_report.html", title="WebSight Sitemap Analysis")
        
        print("\nâœ… Sitemapè§£æå®Œäº†ï¼")
        print(f"   ğŸ“Š {len(sitemap_data.urls)} URLs ç™ºè¦‹")
        print(f"   ğŸ“„ websight_sitemap_report.html\n")
    
    # === ã‚¦ã‚§ãƒ–ã‚¯ãƒ­ãƒ¼ãƒ« ===
    if not args.skip_crawl and args.crawl_url:
        print("\n" + "=" * 60)
        print("ğŸ•·ï¸ ã‚¦ã‚§ãƒ–ã‚¯ãƒ­ãƒ¼ãƒ«")
        print("=" * 60)
        print(f"ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹: {args.crawl_url}")
        print(f"   æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
        print(f"   ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {args.workers}\n")
        
        crawler = WebCrawler(max_workers=args.workers)
        crawler.crawl(args.crawl_url, max_pages=args.max_pages)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
        crawler.data.save_json('websight_crawl_data.json')
        crawler.data.save_csv('websight_crawl_data.csv')
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("\nğŸ¨ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        visualizer = SitemapVisualizer(crawler.data)
        visualizer.generate_html_report('websight_crawl_report.html')
        
        print("\nâœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼")
        print(f"   ğŸ“Š {len(crawler.data.urls)} ãƒšãƒ¼ã‚¸ç™ºè¦‹")
        print(f"   ğŸ“„ websight_crawl_report.html\n")
    
    # === çµ±åˆã‚µãƒãƒªãƒ¼ ===
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    if not args.skip_sitemap:
        print("ğŸ“„ websight_sitemap_report.html - Sitemap.xmlè§£æãƒ¬ãƒãƒ¼ãƒˆ")
    if not args.skip_crawl and args.crawl_url:
        print("ğŸ“„ websight_crawl_report.html - ã‚¦ã‚§ãƒ–ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ")
    print()

if __name__ == "__main__":
    main()
