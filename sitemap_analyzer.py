"""
ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è§£æã®ã‚³ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å–å¾—ã€ãƒ‘ãƒ¼ã‚¹ã€å†å¸°çš„ãªå‡¦ç†ã‚’è¡Œã†
"""
import requests
import xml.etree.ElementTree as ET
from typing import List, Optional, Set
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time
import concurrent.futures
from bs4 import BeautifulSoup

from sitemap_data import SitemapData


class SitemapAnalyzer:
    """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è§£æã‚¯ãƒ©ã‚¹"""
    
    # XMLãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹
    NS = {
        'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9',
        'xhtml': 'http://www.w3.org/1999/xhtml',
        'image': 'http://www.google.com/schemas/sitemap-image/1.1',
        'video': 'http://www.google.com/schemas/sitemap-video/1.1'
    }
    
    def __init__(self, user_agent: str = 'SitemapAnalyzer/1.0', check_links: bool = False):
        self.user_agent = user_agent
        self.check_links = check_links
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': user_agent})
        self.processed_sitemaps: Set[str] = set()
    
    def fetch_sitemap(self, url: str, timeout: int = 30) -> Optional[str]:
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’HTTPã§å–å¾—"""
        try:
            print(f"ğŸ“¡ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’å–å¾—ä¸­: {url}")
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # Content-Typeã‚’ç¢ºèª
            content_type = response.headers.get('Content-Type', '')
            if 'xml' not in content_type and 'text' not in content_type:
                print(f"âš ï¸  è­¦å‘Š: äºˆæœŸã—ãªã„Content-Type: {content_type}")
            
            return response.text
        
        except requests.exceptions.RequestException as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    def check_url_status(self, url: str, timeout: int = 10) -> Optional[int]:
        """URLã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª"""
        if not self.check_links:
            return None
            
        try:
            # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ç¢ºèª
            response = self.session.head(url, timeout=timeout, allow_redirects=True)
            
            # 405 Method Not Allowedã®å ´åˆã¯GETã§å†è©¦è¡Œ
            if response.status_code == 405:
                response = self.session.get(url, timeout=timeout, stream=True)
                response.close()
                
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(0.1)
            
            if response.status_code >= 400:
                print(f"âš ï¸  ãƒªãƒ³ã‚¯åˆ‡ã‚Œ ({response.status_code}): {url}")
            
            return response.status_code
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return None
    
    def parse_sitemap_index(self, xml_content: str) -> List[str]:
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€å«ã¾ã‚Œã‚‹ã‚µã‚¤ãƒˆãƒãƒƒãƒ—URLã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        try:
            root = ET.fromstring(xml_content)
            
            # sitemapindexè¦ç´ ã‚’ç¢ºèª
            if root.tag.endswith('sitemapindex'):
                sitemap_urls = []
                for sitemap in root.findall('sm:sitemap', self.NS):
                    loc = sitemap.find('sm:loc', self.NS)
                    if loc is not None and loc.text:
                        sitemap_urls.append(loc.text.strip())
                
                if sitemap_urls:
                    print(f"ğŸ“‘ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œå‡º: {len(sitemap_urls)} å€‹ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—")
                    return sitemap_urls
            
            return []
        
        except ET.ParseError as e:
            print(f"âŒ XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def parse_urlset(self, xml_content: str, data: SitemapData, sitemap_url: str):
        """URLã‚»ãƒƒãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦SitemapDataã«è¿½åŠ """
        try:
            root = ET.fromstring(xml_content)
            
            # urlsetè¦ç´ ã‚’ç¢ºèª
            if not root.tag.endswith('urlset'):
                return
            
            url_count = 0
            for url_elem in root.findall('sm:url', self.NS):
                loc = url_elem.find('sm:loc', self.NS)
                if loc is None or not loc.text:
                    continue
                
                url = loc.text.strip()
                
                # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                lastmod_elem = url_elem.find('sm:lastmod', self.NS)
                lastmod = lastmod_elem.text.strip() if lastmod_elem is not None and lastmod_elem.text else None
                
                changefreq_elem = url_elem.find('sm:changefreq', self.NS)
                changefreq = changefreq_elem.text.strip() if changefreq_elem is not None and changefreq_elem.text else None
                
                priority_elem = url_elem.find('sm:priority', self.NS)
                priority = None
                if priority_elem is not None and priority_elem.text:
                    try:
                        priority = float(priority_elem.text.strip())
                    except ValueError:
                        pass
                
                # ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯ã¯å¾Œã§ã¾ã¨ã‚ã¦ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—
                # status_code = self.check_url_status(url)
                
                data.add_url(url, lastmod, changefreq, priority, source_sitemap=sitemap_url)
                url_count += 1
            
            print(f"âœ“ {url_count} å€‹ã®URLã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        except ET.ParseError as e:
            print(f"âŒ XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze(self, sitemap_url: str, max_depth: int = 10) -> SitemapData:
        """
        ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è§£æã—ã¦SitemapDataã‚’è¿”ã™
        ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã¯å†å¸°çš„ã«å‡¦ç†
        """
        data = SitemapData()
        data.source_url = sitemap_url
        data.fetched_at = datetime.now().isoformat()
        
        self.processed_sitemaps.clear()
        self._analyze_recursive(sitemap_url, data, depth=0, max_depth=max_depth)
        
        # ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if self.check_links:
            self._check_links_parallel(data)
            # å‚ç…§å…ƒèª¿æŸ»
            self.find_referrers(data)
        
        return data
    
    def _check_links_parallel(self, data: SitemapData, max_workers: int = 10):
        """ä¸¦åˆ—ã§ãƒªãƒ³ã‚¯åˆ‡ã‚Œã‚’ãƒã‚§ãƒƒã‚¯"""
        urls = data.urls
        total = len(urls)
        print(f"ğŸ” {total} ä»¶ã®URLã‚’ä¸¦åˆ—ãƒã‚§ãƒƒã‚¯ä¸­ (max_workers={max_workers})...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # URLã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨Futureã®ãƒãƒƒãƒ”ãƒ³ã‚°
            future_to_url = {executor.submit(self.check_url_status, url_obj.url): url_obj for url_obj in urls}
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                url_obj = future_to_url[future]
                try:
                    status_code = future.result()
                    url_obj.status_code = status_code
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {url_obj.url} - {e}")
                
                completed += 1
                if completed % 10 == 0 or completed == total:
                    print(f"\r[{completed}/{total}] Checking...", end="", flush=True)
            
            print() # æ”¹è¡Œ

    def find_referrers(self, data: SitemapData, max_workers: int = 10):
        """ãƒªãƒ³ã‚¯åˆ‡ã‚ŒURLã®å‚ç…§å…ƒãƒšãƒ¼ã‚¸ã‚’ç‰¹å®šã™ã‚‹"""
        # ãƒªãƒ³ã‚¯åˆ‡ã‚ŒURLã‚’ç‰¹å®š
        broken_urls = {u.url: u for u in data.urls if u.status_code and u.status_code >= 400}
        if not broken_urls:
            return

        # èª¿æŸ»å¯¾è±¡ã®æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ï¼ˆHTMLã®ã¿ï¼‰
        valid_pages = [
            u for u in data.urls 
            if u.status_code and u.status_code == 200 
            and (u.url.endswith('.html') or u.url.endswith('/'))
        ]
        
        total = len(valid_pages)
        print(f"ğŸ•µï¸ {total} ä»¶ã®æœ‰åŠ¹ãªãƒšãƒ¼ã‚¸ã‹ã‚‰å‚ç…§å…ƒã‚’èª¿æŸ»ä¸­...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self._scan_page_for_links, page.url, broken_urls.keys()): page for page in valid_pages}
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                page = future_to_url[future]
                try:
                    found_links = future.result()
                    for broken_link in found_links:
                        if broken_link in broken_urls:
                            broken_urls[broken_link].referrers.append(page.url)
                except Exception as e:
                    print(f"âŒ å‚ç…§å…ƒèª¿æŸ»ã‚¨ãƒ©ãƒ¼: {page.url} - {e}")
                
                completed += 1
                if completed % 10 == 0 or completed == total:
                    print(f"\r[{completed}/{total}] Scanning...", end="", flush=True)
        print()

    def _scan_page_for_links(self, page_url: str, target_urls: Set[str]) -> Set[str]:
        """ãƒšãƒ¼ã‚¸å†…ã®ãƒªãƒ³ã‚¯ã‚’è§£æã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
        found_targets = set()
        try:
            response = self.session.get(page_url, timeout=10)
            if response.status_code != 200:
                return found_targets
                
            soup = BeautifulSoup(response.content, 'html.parser')
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                absolute_url = urljoin(page_url, href)
                
                # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆ(#)ã‚’é™¤å»
                absolute_url = absolute_url.split('#')[0]
                
                if absolute_url in target_urls:
                    found_targets.add(absolute_url)
                    
        except Exception:
            pass
            
        return found_targets
    
    def _analyze_recursive(self, url: str, data: SitemapData, depth: int, max_depth: int):
        """å†å¸°çš„ã«ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è§£æ"""
        # æ·±ã•åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if depth > max_depth:
            print(f"âš ï¸  è­¦å‘Š: æœ€å¤§æ·±åº¦ {max_depth} ã«é”ã—ã¾ã—ãŸ")
            return
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        if url in self.processed_sitemaps:
            print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«å‡¦ç†æ¸ˆã¿ {url}")
            return
        
        self.processed_sitemaps.add(url)
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’å–å¾—
        xml_content = self.fetch_sitemap(url)
        if not xml_content:
            return
        
        # å°‘ã—å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
        time.sleep(0.5)
        
        # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
        sitemap_urls = self.parse_sitemap_index(xml_content)
        
        if sitemap_urls:
            # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã€å„ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’å†å¸°çš„ã«å‡¦ç†
            for sitemap_url in sitemap_urls:
                self._analyze_recursive(sitemap_url, data, depth + 1, max_depth)
        else:
            # é€šå¸¸ã®URLã‚»ãƒƒãƒˆã¨ã—ã¦å‡¦ç†
            self.parse_urlset(xml_content, data, url)
    
    def discover_sitemap(self, domain: str) -> Optional[str]:
        """
        ä¸€èˆ¬çš„ãªã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å ´æ‰€ã‚’è©¦ã—ã¦ã€ã‚µã‚¤ãƒˆãƒãƒƒãƒ—URLã‚’è‡ªå‹•æ¤œå‡º
        """
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æ­£è¦åŒ–
        if not domain.startswith('http'):
            domain = 'https://' + domain
        
        parsed = urlparse(domain)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        # è©¦ã™ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        common_paths = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemap1.xml',
            '/sitemaps/sitemap.xml',
        ]
        
        print(f"ğŸ” ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è‡ªå‹•æ¤œå‡ºä¸­: {base_url}")
        
        for path in common_paths:
            url = base_url + path
            try:
                response = self.session.head(url, timeout=10, allow_redirects=True)
                if response.status_code == 200:
                    print(f"âœ“ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç™ºè¦‹: {url}")
                    return url
            except requests.exceptions.RequestException:
                continue
        
        # robots.txtã‹ã‚‰æ¤œå‡ºã‚’è©¦ã¿ã‚‹
        try:
            robots_url = base_url + '/robots.txt'
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                for line in response.text.split('\n'):
                    if line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        print(f"âœ“ robots.txtã‹ã‚‰ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç™ºè¦‹: {sitemap_url}")
                        return sitemap_url
        except requests.exceptions.RequestException:
            pass
        
        print("âŒ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è‡ªå‹•æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None
