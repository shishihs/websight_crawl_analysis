"""
ãƒ©ã‚¤ãƒ•ãƒãƒƒãƒˆç”Ÿå‘½ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import argparse
from web_crawler import WebCrawler
from sitemap_visualizer import SitemapVisualizer
from datetime import datetime

def main():
    parser = argparse.ArgumentParser(description='ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ§‹é€ è§£æã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼')
    parser.add_argument('--url', type=str, default="https://www.lifenet-seimei.co.jp/", help='é–‹å§‹URL')
    parser.add_argument('--max-pages', type=int, default=500, help='æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°')
    parser.add_argument('--workers', type=int, default=10, help='ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°')
    args = parser.parse_args()
    
    start_url = args.url
    
    print(f"ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™: {start_url}")
    print(f"   æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {args.max_pages}")
    print(f"   ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {args.workers}")
    
    # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–ã¨å®Ÿè¡Œ
    crawler = WebCrawler(
        start_url=start_url, 
        max_pages=args.max_pages, 
        max_workers=args.workers
    )
    data = crawler.crawl()
    
    data.fetched_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...")
    data.save_json("lifenet_crawl_data.json")
    data.save_csv("lifenet_crawl_data.csv")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ¨ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    visualizer = SitemapVisualizer(data)
    visualizer.generate_html_report("lifenet_crawl_report.html", title="ãƒ©ã‚¤ãƒ•ãƒãƒƒãƒˆç”Ÿå‘½ ã‚µã‚¤ãƒˆæ§‹é€ è§£æãƒ¬ãƒãƒ¼ãƒˆ")
    
    print("\nâœ… å®Œäº†ã—ã¾ã—ãŸï¼")
    print("   open lifenet_crawl_report.html")

if __name__ == "__main__":
    main()
